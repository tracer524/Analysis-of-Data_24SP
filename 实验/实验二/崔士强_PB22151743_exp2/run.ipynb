{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析及实践 实验二\n",
    "#### 崔士强 PB22151743\n",
    "下面每一个代码框对应一项任务，共5个."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import json\n",
    "\n",
    "url = \"https://dblp.dagstuhl.de/db/conf/kdd/kdd2023.html\"\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "# 解码，获得字符串\n",
    "html = response.read().decode(\"utf-8\")\n",
    "\n",
    "# 将字符串写入文件\n",
    "with open(\"page.txt\", \"w\") as file:\n",
    "    file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Track Full Papers\n",
      "Applied Data Track Full Papers\n",
      "Hands On Tutorials\n",
      "Lecture Style Tutorials\n",
      "Workshop Summaries\n"
     ]
    }
   ],
   "source": [
    "with open(\"page.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "# 找到网站页面内容对应的行\n",
    "for line in lines:\n",
    "    if(\"Applied Data Track Full Papers\" in line):\n",
    "        i = lines.index(line)\n",
    "\n",
    "# 标题格式\n",
    "title_pattern = '<h2 id=.{1,100}>.{1,100}</h2></header>'\n",
    "\n",
    "# 查找所有标题所在的语句\n",
    "titleList = re.findall(title_pattern, lines[i])\n",
    "\n",
    "# 逐个提取标题\n",
    "for title in titleList:\n",
    "    temp = re.sub('<h2 id=.+\\\">', '', title)\n",
    "    print(re.sub('</h2></header>', '', temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "strList = lines[i].split(\"<header class=\\\"h2\\\">\")\n",
    "\n",
    "track1dict = {}\n",
    "track2dict = {}\n",
    "trackDictList = [track1dict, track2dict]\n",
    "\n",
    "track1dict[\"track\"] = \"Research Track Full Papers\"\n",
    "track1dict[\"papers\"] = []\n",
    "track2dict[\"track\"] = \"Applied Data Track Full Papers\"\n",
    "track2dict[\"papers\"] = []\n",
    "\n",
    "for track_no in range(1,3):\n",
    "    # 以论文为单位初步分割\n",
    "    temp_list = strList[track_no].split(\"<meta property=\\\"genre\\\" content=\\\"computer science\\\">\")\n",
    "    for str in temp_list:\n",
    "        paper_dict = {}\n",
    "        try:\n",
    "            # 查找所有作者\n",
    "            author_pattern = 'itemprop=\\\"name\\\" title=\\\"[a-zA-Z -]+\\\"'\n",
    "            authorList = re.findall(author_pattern, str)\n",
    "            paper_dict[\"author\"] = [author.split('\\\"')[-2] for author in authorList]\n",
    "            \n",
    "            # 查找标题\n",
    "            paper_dict[\"title\"] = str.split(\"<span class=\\\"title\\\" itemprop=\\\"name\\\">\")[1].split(\"</span>\")[0]\n",
    "            \n",
    "            # 查找收录起始页和终止页\n",
    "            paper_dict[\"startPage\"] = int(str.split(\"<span itemprop=\\\"pagination\\\">\")[1].split(\"-\")[0])\n",
    "            paper_dict[\"endPage\"] = int(str.split(\"<span itemprop=\\\"pagination\\\">\")[1].split(\"-\")[1].split(\"</span>\")[0])\n",
    "            \n",
    "            # 存进对应track的papers列表\n",
    "            if(track_no == 1):\n",
    "                track1dict[\"papers\"].append(paper_dict)\n",
    "            else:\n",
    "                track2dict[\"papers\"].append(paper_dict)\n",
    "        except:\n",
    "            continue\n",
    "   \n",
    "print(len(track1dict[\"papers\"]))\n",
    "print(len(track2dict[\"papers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kdd23.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(trackDictList, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florian Adriaens\n",
      "Honglian Wang\n",
      "Aristides Gionis\n",
      "Rishi Advani\n",
      "Paolo Papotti\n",
      "Abolfazl Asudeh\n",
      "Amine Allouah\n",
      "Christian Kroer\n",
      "Xuan Zhang\n",
      "Vashist Avadhanula\n",
      "Nona Bohanon\n",
      "Anil Dania\n",
      "Caner Gocmen\n",
      "Sergey Pupyrev\n",
      "Parikshit Shah\n",
      "Nicol&#225;s Stier Moses\n",
      "Ken Rodr&#237;guez Taarup\n",
      "Mario Almagro\n",
      "Emilio J. Almaz&#225;n\n",
      "Diego Ortego\n",
      "David Jim&#233;nez\n",
      "Amel Awadelkarim\n",
      "Arjun Seshadri\n",
      "Itai Ashlagi\n",
      "Irene Lo\n",
      "Johan Ugander\n",
      "Jiaxin Bai\n",
      "Chen Luo\n",
      "Zheng Li\n",
      "Qingyu Yin\n",
      "Bing Yin\n",
      "Yangqiu Song\n",
      "Ergute Bao\n",
      "Dawei Gao\n",
      "Xiaokui Xiao\n",
      "Yaliang Li\n",
      "Anna Beer\n",
      "Andrew Draganov\n",
      "Ellen Hohma\n",
      "Philipp Jahn\n",
      "Christian M. M. Frey\n",
      "Ira Assent\n",
      "Siddharth Bhatia\n",
      "Mohit Wadhwa\n",
      "Kenji Kawaguchi\n",
      "Neil Shah\n",
      "Philip S. Yu\n",
      "Bryan Hooi\n",
      "Adam Breuer\n",
      "Nazanin Khosravani Tehrani\n",
      "Michael Tingley\n",
      "Bradford Cottel\n",
      "Abhinav Anand\n",
      "Surender Kumar\n",
      "Nandeesh Kumar\n",
      "Samir Shah\n",
      "Wenxuan Ao\n",
      "Guozhen Zhang\n",
      "Yong Li\n",
      "Depeng Jin\n",
      "Yash Kumar Atri\n",
      "Vikram Goyal\n",
      "Tanmoy Chakraborty\n",
      "Vasilisa Bashlovkina\n",
      "Riley Matthews\n",
      "Zhaobin Kuang\n",
      "Simon Baumgartner\n",
      "Michael Bendersky\n",
      "Xi Jiang\n",
      "Van Tran\n",
      "Arjun Nitin Bhagoji\n",
      "Nguyen Phong Hoang\n",
      "Nick Feamster\n",
      "Prateek Mittal\n",
      "Vinod Yegneswaran\n",
      "Maarten Buyl\n",
      "Paul Missault\n",
      "Pierre-Antoine Sondag\n",
      "Zhangming Chan\n",
      "Yu Zhang\n",
      "Shuguang Han\n",
      "Yong Bai\n",
      "Xiang-Rong Sheng\n",
      "Siyuan Lou\n",
      "Jiacen Hu\n",
      "Baolin Liu\n",
      "Yuning Jiang\n",
      "Jian Xu\n",
      "Bo Zheng\n",
      "Jianxin Chang\n",
      "Chenbin Zhang\n",
      "Zhiyi Fu\n",
      "Xiaoxue Zang\n",
      "Lin Guan\n",
      "Jing Lu\n",
      "Yiqun Hui\n",
      "Dewei Leng\n",
      "Yanan Niu\n",
      "Yang Song\n",
      "Kun Gai\n",
      "Xiaomin Chang\n",
      "Wei Li\n",
      "Yunchuan Shi\n",
      "Albert Y. Zomaya\n"
     ]
    }
   ],
   "source": [
    "def getPapers(str):\n",
    "    html = urllib.request.urlopen(str).read().decode(\"utf-8\")\n",
    "    paper_dict_list = []\n",
    "    content = html.split(\"class=\\\"hide-head h2\\\">\")[2]\n",
    "    str_list = content.split(\"<meta property=\\\"genre\\\" content=\\\"computer science\\\">\")\n",
    "    for str in str_list:\n",
    "        try:\n",
    "            paper_dict = {}\n",
    "            authors = re.findall(\"<span itemprop=\\\"name\\\" title=\\\"[^\\\"]+\\\">[^<]+|<span class=\\\"this-person\\\" itemprop=\\\"name\\\">[^<]+\", str)\n",
    "            author_list = []\n",
    "            for author in authors:\n",
    "                author_list.append(author.split(\">\")[-1])\n",
    "            paper_dict[\"authors\"] = author_list\n",
    "            \n",
    "            title = re.search(\"<span class=\\\"title\\\" itemprop=\\\"name\\\">[^<]+\", str).group().split(\">\")[-1]\n",
    "            paper_dict[\"title\"] = title\n",
    "            \n",
    "            info_name = re.search(\"<span itemprop=\\\"isPartOf\\\" itemscope itemtype=\\\"[^\\\"]+\\\"><span itemprop=\\\"name\\\">[^<]+\", str).group().split(\">\")[-1]\n",
    "            try:\n",
    "                info_volume = re.search(\"<span itemprop=\\\"volumeNumber\\\">[^<]+\", str).group().split(\">\")[-1]\n",
    "                info_name = info_name + \" \" + info_volume\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                info_issue = re.search(\"<span itemprop=\\\"issueNumber\\\">[^<]+\", str).group().split(\">\")[-1]\n",
    "                info_name = info_name + \"(\" + info_issue + \")\"\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                info_pagination = re.search(\"<span itemprop=\\\"pagination\\\">[^<]+\", str).group().split(\">\")[-1]\n",
    "                info_name = info_name + \": \" + info_pagination\n",
    "            except:\n",
    "                pass\n",
    "            paper_dict[\"publishInfo\"] = info_name\n",
    "                \n",
    "            datePublished = re.search(\"<span itemprop=\\\"datePublished\\\">[0-9]{4}\", str).group().split(\">\")[-1]\n",
    "            paper_dict[\"year\"] = datePublished\n",
    "            \n",
    "            paper_dict_list.append(paper_dict)\n",
    "        except:\n",
    "            continue\n",
    "    return paper_dict_list\n",
    "\n",
    "researcherDictList = []\n",
    "researcher_pattern = '<span itemprop=\\\"author\\\" itemscope itemtype=\\\"http://schema.org/Person\\\"><a href=\\\"[^\\\"]+\\\" itemprop=\\\"url\\\"><span itemprop=\\\"name\\\" title=\\\"[^\\\"]+\\\">[^<]+</span></a><img src=\\\"[^\\\"]+\\\" style=\\\"padding-left:0.25em;\\\" alt=\\\"\\\" title=\\\"[^\\\"]{19}'\n",
    "for track_no in range(1,3):\n",
    "    # 截取前10篇论文\n",
    "    temp_list = strList[track_no].split(\"<meta property=\\\"genre\\\" content=\\\"computer science\\\">\")[0:10]\n",
    "    for str in temp_list:\n",
    "        try:\n",
    "            # 找到每个包含研究者信息的字符串\n",
    "            researcherList = re.findall(researcher_pattern, str)\n",
    "            \n",
    "            for res_str in researcherList:\n",
    "                researcher_dict = {}\n",
    "                name = re.search('itemprop=\\\"name\\\" title=\\\"[^\\\"]+\\\">[^<]+', res_str).group().split(\">\")[-1]\n",
    "                \n",
    "                # 查重\n",
    "                hasDuplicate = False\n",
    "                for dic in researcherDictList:\n",
    "                    if(dic[\"researcher\"] == name):\n",
    "                        hasDuplicate = True\n",
    "                        break\n",
    "                if(hasDuplicate):\n",
    "                    continue\n",
    "                \n",
    "                researcher_dict[\"researcher\"] = name\n",
    "                \n",
    "                researcher_dict[\"orcID\"] = re.search('[0-9-X]{19}', res_str).group()\n",
    "                \n",
    "                researcher_html = re.search('https://[^\"]+\\.html', res_str).group()\n",
    "                \n",
    "                researcher_dict[\"papers\"] = getPapers(researcher_html)\n",
    "                \n",
    "                researcherDictList.append(researcher_dict)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "with open('researchers.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(researcherDictList, json_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
