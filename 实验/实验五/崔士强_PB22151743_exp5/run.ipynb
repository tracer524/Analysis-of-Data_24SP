{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验五"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "崔士强 PB22151743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验三数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用实验三的数据，对`diagnosis`进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          568 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   567 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               567 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           568 non-null    float64\n",
      " 18  concavity_se             568 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     568 non-null    float64\n",
      " 22  radius_worst             568 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         568 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n",
      "None\n",
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
      "0  ...         25.38          17.33           184.60      2019.0   \n",
      "1  ...         24.99          23.41           158.80      1956.0   \n",
      "2  ...         23.57          25.53           152.50      1709.0   \n",
      "3  ...         14.91          26.50            98.87       567.7   \n",
      "4  ...         22.54          16.67           152.20      1575.0   \n",
      "\n",
      "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   symmetry_worst  fractal_dimension_worst  \n",
      "0          0.4601                  0.11890  \n",
      "1          0.2750                  0.08902  \n",
      "2          0.3613                  0.08758  \n",
      "3          0.6638                  0.17300  \n",
      "4          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', encoding='utf-8')\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于主实验，采取下面两种算法\n",
    "\n",
    "1. 支持向量机（SVM）\n",
    "\n",
    "SVM的目的是找到一个超平面，它可以有效地分离不同类别的数据点，同时尽可能地最大化不同类别之间的间隔。SVM对于小到中等数据集表现良好，尤其是在数据维度较高时。\n",
    "\n",
    "参考文献：\n",
    "\n",
    "\t•\tCortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297.\n",
    "\n",
    "1. 梯度提升树（Gradient Boosting Machines, GBM）\n",
    "\n",
    "GBM通过逐步修正前一个模型的残差来增强模型的预测能力，在处理各种统计分类和回归问题时表现出色。GBM特别适用于处理复杂的非线性关系。\n",
    "\n",
    "参考文献：\n",
    "\n",
    "\t•\tFriedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of statistics, 1189-1232."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.94      0.98      0.96        63\n",
      "           M       0.98      0.92      0.95        49\n",
      "\n",
      "    accuracy                           0.96       112\n",
      "   macro avg       0.96      0.95      0.95       112\n",
      "weighted avg       0.96      0.96      0.96       112\n",
      "\n",
      "GBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.94      0.95      0.94        63\n",
      "           M       0.94      0.92      0.93        49\n",
      "\n",
      "    accuracy                           0.94       112\n",
      "   macro avg       0.94      0.94      0.94       112\n",
      "weighted avg       0.94      0.94      0.94       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分离特征和目标变量\n",
    "X = df.drop(['diagnosis', 'id'], axis=1)  # 特征\n",
    "y = df['diagnosis']  # 目标变量\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "svm_model = SVC(kernel='linear', random_state=42)  # 线性核SVM\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "svm_model.fit(X_train, y_train)\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测和评估\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "gbm_pred = gbm_model.predict(X_test)\n",
    "\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "print(\"GBM Classification Report:\")\n",
    "print(classification_report(y_test, gbm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best SVM Score: 0.9620224719101124\n",
      "Best GBM Parameters: {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best GBM Score: 0.966541822721598\n",
      "SVM Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.94      1.00      0.97        63\n",
      "           M       1.00      0.92      0.96        49\n",
      "\n",
      "    accuracy                           0.96       112\n",
      "   macro avg       0.97      0.96      0.96       112\n",
      "weighted avg       0.97      0.96      0.96       112\n",
      "\n",
      "GBM Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.94      0.95      0.94        63\n",
      "           M       0.94      0.92      0.93        49\n",
      "\n",
      "    accuracy                           0.94       112\n",
      "   macro avg       0.94      0.94      0.94       112\n",
      "weighted avg       0.94      0.94      0.94       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置参数网格\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "gbm_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# 创建 GridSearchCV 对象\n",
    "svm_grid = GridSearchCV(SVC(random_state=42), svm_params, cv=5, scoring='accuracy')\n",
    "gbm_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), gbm_params, cv=5, scoring='accuracy')\n",
    "\n",
    "# 执行网格搜索\n",
    "svm_grid.fit(X_train, y_train)\n",
    "gbm_grid.fit(X_train, y_train)\n",
    "\n",
    "# 最佳参数和最佳模型评分\n",
    "print(\"Best SVM Parameters:\", svm_grid.best_params_)\n",
    "print(\"Best SVM Score:\", svm_grid.best_score_)\n",
    "print(\"Best GBM Parameters:\", gbm_grid.best_params_)\n",
    "print(\"Best GBM Score:\", gbm_grid.best_score_)\n",
    "\n",
    "# 使用最佳参数模型进行预测\n",
    "svm_best = svm_grid.best_estimator_.predict(X_test)\n",
    "gbm_best = gbm_grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# 评估\n",
    "print(\"SVM Test Set Performance:\")\n",
    "print(classification_report(y_test, svm_best))\n",
    "\n",
    "print(\"GBM Test Set Performance:\")\n",
    "print(classification_report(y_test, gbm_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的结果基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验四数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用实验四的数据，对`Class`进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据进行预处理，过程包括去除缺失值，转换为独热向量等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "['node-caps', 'breast-quad']\n",
      "Value distribution for 'tumor-size' before corrections:\n",
      "tumor-size\n",
      "30-34     57\n",
      "25-29     51\n",
      "20-24     48\n",
      "15-19     29\n",
      "14-Oct    28\n",
      "40-44     22\n",
      "35-39     19\n",
      "0-4        8\n",
      "50-54      8\n",
      "9-May      4\n",
      "45-49      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value distribution for 'inv-nodes' before corrections:\n",
      "inv-nodes\n",
      "0-2       209\n",
      "5-Mar      34\n",
      "8-Jun      17\n",
      "11-Sep      7\n",
      "15-17       6\n",
      "14-Dec      3\n",
      "24-26       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Corrected value distribution for 'tumor-size':\n",
      "tumor-size\n",
      "30-34    57\n",
      "25-29    51\n",
      "20-24    48\n",
      "15-19    29\n",
      "10-14    28\n",
      "40-44    22\n",
      "35-39    19\n",
      "0-4       8\n",
      "50-54     8\n",
      "5-9       4\n",
      "45-49     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Corrected value distribution for 'inv-nodes':\n",
      "inv-nodes\n",
      "0-2      209\n",
      "3-5       34\n",
      "6-8       17\n",
      "9-11       7\n",
      "15-17      6\n",
      "12-14      3\n",
      "24-26      1\n",
      "Name: count, dtype: int64\n",
      "0 Class=no-recurrence-events\n",
      "1 Class=recurrence-events\n",
      "2 age=10-19\n",
      "3 age=20-29\n",
      "4 age=30-39\n",
      "5 age=40-49\n",
      "6 age=50-59\n",
      "7 age=60-69\n",
      "8 age=70-79\n",
      "9 age=80-89\n",
      "10 age=90-99\n",
      "11 menopause=lt40\n",
      "12 menopause=ge40\n",
      "13 menopause=premeno\n",
      "14 tumor-size=0-4\n",
      "15 tumor-size=5-9\n",
      "16 tumor-size=10-14\n",
      "17 tumor-size=15-19\n",
      "18 tumor-size=20-24\n",
      "19 tumor-size=25-29\n",
      "20 tumor-size=30-34\n",
      "21 tumor-size=35-39\n",
      "22 tumor-size=40-44\n",
      "23 tumor-size=45-49\n",
      "24 tumor-size=50-54\n",
      "25 tumor-size=55-59\n",
      "26 inv-nodes=0-2\n",
      "27 inv-nodes=3-5\n",
      "28 inv-nodes=6-8\n",
      "29 inv-nodes=9-11\n",
      "30 inv-nodes=12-14\n",
      "31 inv-nodes=15-17\n",
      "32 inv-nodes=18-20\n",
      "33 inv-nodes=21-23\n",
      "34 inv-nodes=24-26\n",
      "35 inv-nodes=27-29\n",
      "36 inv-nodes=30-32\n",
      "37 inv-nodes=33-35\n",
      "38 inv-nodes=36-39\n",
      "39 node-caps=yes\n",
      "40 node-caps=no\n",
      "41 deg-malig=1\n",
      "42 deg-malig=2\n",
      "43 deg-malig=3\n",
      "44 breast=left\n",
      "45 breast=right\n",
      "46 breast-quad=left_up\n",
      "47 breast-quad=left_low\n",
      "48 breast-quad=right_up\n",
      "49 breast-quad=right_low\n",
      "50 breast-quad=central\n",
      "51 irradiat=yes\n",
      "52 irradiat=no\n",
      "      id  Class  age  menopause  tumor-size  inv-nodes  node-caps  deg-malig  \\\n",
      "0      0      0    4         13          20         26         40         43   \n",
      "1      1      0    5         13          18         26         40         42   \n",
      "2      2      0    5         13          18         26         40         42   \n",
      "3      3      0    7         12          17         26         40         42   \n",
      "4      4      0    5         13          14         26         40         42   \n",
      "..   ...    ...  ...        ...         ...        ...        ...        ...   \n",
      "281  281      1    4         13          20         26         40         42   \n",
      "282  282      1    4         13          18         26         40         43   \n",
      "283  283      1    7         12          18         26         40         41   \n",
      "284  284      1    5         12          20         27         40         43   \n",
      "285  285      1    6         12          20         27         40         43   \n",
      "\n",
      "     breast  breast-quad  irradiat  \n",
      "0        44           47        52  \n",
      "1        45           48        52  \n",
      "2        44           47        52  \n",
      "3        45           46        52  \n",
      "4        45           49        52  \n",
      "..      ...          ...       ...  \n",
      "281      44           46        52  \n",
      "282      44           46        51  \n",
      "283      45           46        52  \n",
      "284      44           47        52  \n",
      "285      44           47        52  \n",
      "\n",
      "[277 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "columns_with_missing_values = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "print(columns_with_missing_values)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"Value distribution for 'tumor-size' before corrections:\")\n",
    "print(df['tumor-size'].value_counts())\n",
    "\n",
    "print(\"\\nValue distribution for 'inv-nodes' before corrections:\")\n",
    "print(df['inv-nodes'].value_counts())\n",
    "\n",
    "correction_map_tumor_size = {\n",
    "    '14-Oct' : '10-14',\n",
    "    '9-May' : '5-9'\n",
    "}\n",
    "\n",
    "correction_map_inv_nodes = {\n",
    "    '5-Mar' : '3-5',\n",
    "    '8-Jun' : '6-8',\n",
    "    '11-Sep' : '9-11',\n",
    "    '14-Dec' : '12-14'\n",
    "}\n",
    "\n",
    "df['tumor-size'] = df['tumor-size'].replace(correction_map_tumor_size)\n",
    "df['inv-nodes'] = df['inv-nodes'].replace(correction_map_inv_nodes)\n",
    "\n",
    "print(\"\\nCorrected value distribution for 'tumor-size':\")\n",
    "print(df['tumor-size'].value_counts())\n",
    "\n",
    "print(\"\\nCorrected value distribution for 'inv-nodes':\")\n",
    "print(df['inv-nodes'].value_counts())\n",
    "\n",
    "\n",
    "variables_df = pd.read_excel('variables.xlsx')\n",
    "\n",
    "ind2val = {}\n",
    "current_index = 0\n",
    "\n",
    "for i, row in variables_df.iterrows():\n",
    "    variable_name = row['Variable Name']\n",
    "    values = row['Description'].split(', ')\n",
    "    for value in values:\n",
    "        ind2val[current_index] = f\"{variable_name}={value}\"\n",
    "        current_index += 1\n",
    "\n",
    "val2ind = {v: k for k, v in ind2val.items()}\n",
    "\n",
    "for column in df.columns:\n",
    "    if column in variables_df['Variable Name'].values:\n",
    "        # 为每个列值构建一个映射函数\n",
    "        df[column] = df[column].apply(lambda x: val2ind[f\"{column}={x}\"])\n",
    "\n",
    "# 打印ind2val字典\n",
    "for key in list(ind2val.keys()):\n",
    "    print(key, ind2val[key])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们选择两种算法模型进行比较：\n",
    "\n",
    "\t• 逻辑回归：广泛用于二分类问题的线性模型，它预测的是观察属于特定类别的概率。参考资料: Hosmer, D. W., & Lemeshow, S. (2000). Applied Logistic Regression.\n",
    "\t• 随机森林：一个基于决策树的集成学习算法，通过构建多棵树并取它们的多数投票来提高预测准确性和稳定性。参考资料: Breiman, L. (2001). Random Forests. Machine Learning.\n",
    "\n",
    "我们使用所有可用特征，因为逻辑回归和随机森林能够较好地处理高维数据，并从中选择重要的特征。\n",
    "\n",
    "在这一步，第一次尝试的结果是准确度达到100%（在这里发现的原因是我先做了实验四数据），更换多种方法及模型均无果。最终发现原因是没有把id从特征中去除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型评估结果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.82        37\n",
      "           1       0.75      0.32      0.44        19\n",
      "\n",
      "    accuracy                           0.73        56\n",
      "   macro avg       0.74      0.63      0.63        56\n",
      "weighted avg       0.74      0.73      0.69        56\n",
      "\n",
      "随机森林模型评估结果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        37\n",
      "           1       0.43      0.32      0.36        19\n",
      "\n",
      "    accuracy                           0.62        56\n",
      "   macro avg       0.56      0.55      0.55        56\n",
      "weighted avg       0.60      0.62      0.61        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分离特征和目标变量\n",
    "X = df.drop(['Class', 'id'], axis=1)  # 特征\n",
    "y = df['Class']  # 目标变量\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 训练逻辑回归\n",
    "logreg_pipeline = Pipeline(steps=[('classifier', logreg)])\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "\n",
    "# 训练随机森林\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# 打印逻辑回归模型的评估结果\n",
    "print(\"逻辑回归模型评估结果:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# 打印随机森林模型的评估结果\n",
    "print(\"随机森林模型评估结果:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试使用k折交叉验证，结果如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归交叉验证评估结果:\n",
      "fit_time: 0.006 (+/- 0.009)\n",
      "score_time: 0.002 (+/- 0.000)\n",
      "test_accuracy: 0.736 (+/- 0.039)\n",
      "test_precision: 0.720 (+/- 0.044)\n",
      "test_recall: 0.736 (+/- 0.039)\n",
      "test_f1: 0.702 (+/- 0.039)\n",
      "\n",
      "随机森林交叉验证评估结果:\n",
      "fit_time: 0.041 (+/- 0.001)\n",
      "score_time: 0.005 (+/- 0.001)\n",
      "test_accuracy: 0.736 (+/- 0.027)\n",
      "test_precision: 0.717 (+/- 0.040)\n",
      "test_recall: 0.736 (+/- 0.027)\n",
      "test_f1: 0.715 (+/- 0.049)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = df.drop(['Class', 'id'], axis=1)  # 特征\n",
    "y = df['Class']  # 目标变量\n",
    "\n",
    "# 初始化模型\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 定义交叉验证策略\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 定义评估指标\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# 执行逻辑回归的交叉验证\n",
    "logreg_scores = cross_validate(logreg, X, y, cv=cv_strategy, scoring=scoring_metrics)\n",
    "print(\"逻辑回归交叉验证评估结果:\")\n",
    "for metric, scores in logreg_scores.items():\n",
    "    print(f\"{metric}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "# 执行随机森林的交叉验证\n",
    "rf_scores = cross_validate(rf, X, y, cv=cv_strategy, scoring=scoring_metrics)\n",
    "print(\"\\n随机森林交叉验证评估结果:\")\n",
    "for metric, scores in rf_scores.items():\n",
    "    print(f\"{metric}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳参数组合: {'max_depth': 10, 'n_estimators': 50}\n",
      "最佳交叉验证分数: 0.7919289152165865\n",
      "最佳模型评估结果:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.84      0.76        37\n",
      "           1       0.45      0.26      0.33        19\n",
      "\n",
      "    accuracy                           0.64        56\n",
      "   macro avg       0.57      0.55      0.54        56\n",
      "weighted avg       0.61      0.64      0.61        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 参数网格搜索\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 使用最佳模型进行预测和评估\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "y_pred_best_rf = grid_search.predict(X_test)\n",
    "best_rf_scores = classification_report(y_test, y_pred_best_rf, output_dict=True)\n",
    "\n",
    "# 打印最佳参数和最佳分数\n",
    "print(\"最佳参数组合:\", best_params)\n",
    "print(\"最佳交叉验证分数:\", best_score)\n",
    "\n",
    "# 打印使用最佳模型的评估结果\n",
    "print(\"最佳模型评估结果:\")\n",
    "print(classification_report(y_test, y_pred_best_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略有提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
